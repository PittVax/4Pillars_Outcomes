{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import xlrd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "# Configure Pandas options\n",
    "import locale\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# Set display options for dataframes output as strings\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "locale.setlocale(locale.LC_NUMERIC, '')\n",
    "\n",
    "\n",
    "# Use seaborn for platting\n",
    "import seaborn as sns; sns.set()\n",
    "# Configure logging\n",
    "import logging\n",
    "import sys\n",
    "# Create logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "# Create STDERR handler\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "formatter = logging.Formatter('%(levelname)s - %(funcName)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "# Set STDERR handler as the only handler \n",
    "logger.handlers = [handler]\n",
    "# ### Examples\n",
    "# ```\n",
    "# # Set logging level\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "# # Example to defer output\n",
    "# if logger.isEnabledFor(logging.DEBUG):\n",
    "#      logger.debug(display(df))\n",
    "#  ```\n",
    "\n",
    "# ## Configure debugging\n",
    "import pdb\n",
    "# Toggole automatic debugging at errors\n",
    "get_ipython().run_line_magic('pdb', 'off')\n",
    "# Set error display\n",
    "get_ipython().run_line_magic('xmode', 'plain')\n",
    "# ### Usage\n",
    "# ```\n",
    "# # Set a breakpoint\n",
    "# pdb.set_trace()\n",
    "# # open shell at error\n",
    "# pdb.pm()\n",
    "# ```\n",
    "\n",
    "# Set project paths\n",
    "home = os.path.expanduser(\"~\")\n",
    "print('home = ' + home)\n",
    "\n",
    "projectRoot = os.path.dirname(os.getcwd())\n",
    "print('projectRoot = ' + projectRoot)\n",
    "\n",
    "# Set path to Box Sync\n",
    "if os.path.isdir(Path(home, 'Box Sync')):\n",
    "    dirBox = Path(home, 'Box Sync')\n",
    "elif os.path.isdir(Path(home, 'Documents', 'Box Sync')):\n",
    "    dirBox = Path(home, 'Documents', 'Box Sync')\n",
    "else:\n",
    "    print('Cannot locate Box Sync directory.')\n",
    "print('dirBox = ' + str(dirBox))\n",
    "\n",
    "# Set paths\n",
    "dirDataPrivate = Path(dirBox, 'data', 'cmi')\n",
    "print('dirDataPrivate = ' + str(dirDataPrivate))\n",
    "\n",
    "dirDataPublic = Path(projectRoot, 'data')\n",
    "print('dirDataPublic = ' + str(dirDataPublic))\n",
    "\n",
    "dirDataEpic = Path(dirDataPrivate, 'epic')\n",
    "print('dirDataEpic = ' + str(dirDataEpic))\n",
    "\n",
    "dicts = Path(dirDataEpic, 'secrets')\n",
    "sys.path.append(os.path.abspath(dicts))\n",
    "import dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes about the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following are identified for each vaccine pre-intervention file:  \n",
    "\n",
    "* each patient seen during the preintervention time frame  \n",
    "* the first visit the patient had to any Department within the Location during the time frame  \n",
    "* the number of encounters to the Location during the time frame  \n",
    "* the last date the vaccine was given as long as that date is prior to the end of the baseline time frame  \n",
    "* if the date of the vaccine matches the visit date, there is a Yes in the column indicating the vaccine was given at the visit  \n",
    "* if the date of the vaccine was prior to the first baseline visit, there is a Yes in the column indicating the vaccine was given before the first visit  \n",
    "\n",
    "## The following are identified for each vaccine post-intervention file:  \n",
    "\n",
    "* each patient seen during the post intervention time frame  \n",
    "* the first visit the patient had to any Department within the Location during the time frame  \n",
    "* number of encounters to the Location during the time frame - if a patient was seen in multiple departments, each department will show the same number of visits  \n",
    "* the last date the vaccine was given as long as that date is prior to the end of the post-intervention time frame  \n",
    "* For Td/Tdap and Pneumo/PCV, the dates of the vaccines will allow the analysis to include what was given first and the interval between the vaccines  \n",
    "* if the date of the vaccine matches the visit date, there is a Yes in the column indicating the vaccine was given at the visit  \n",
    "* if the date of the vaccine was prior to the first baseline visit, there is a Yes in the column indicating the vaccine was given before the first visit   \n",
    "\n",
    "## Patients will appear multiple times for the following reasons:\n",
    "\n",
    "* Their age changed during the time frame so they appear as Age 60-64 and as 65+  \n",
    "* They were seen in more than one department within the location  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_raw_data(file=None, convert=None):\n",
    "    '''Loads raw data from .xlsx files or available pickle to a dictionay.\n",
    "\n",
    "        Args:\n",
    "            file (str): Name of file for saving and retrieving pickled data.\n",
    "                Defaults to 'rawData.p'\n",
    "            convert (bool): Specifies reading from .xlsx when True otherwise, data is\n",
    "                read from pickle if available. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            di (dict): Dictionary of dataframes indexed by filename.\n",
    "\n",
    "    '''\n",
    "    if file == None:\n",
    "        file = 'rawData.p'\n",
    "    else:\n",
    "        file = file + '.p'\n",
    "    if (convert == True) | (os.path.isfile(os.path.join(dirDataEpic,\n",
    "                                                        file)) == False):\n",
    "\n",
    "        # Create dict of dataframes indexed by filename\n",
    "        files = [f for f in os.listdir(dirDataEpic) if f.endswith('.xlsx')]\n",
    "        logger.info('Reading raw data.')\n",
    "        di = {\n",
    "            'df' + file.replace(\".xlsx\", \"\"): pd.read_excel(\n",
    "                os.path.join(dirDataEpic, file))\n",
    "            for file in files\n",
    "        }\n",
    "        save_results_to_pickle(obj=di, file=file, publicPath=dirDataPrivate, privatePath=dirDataEpic)\n",
    "    else:\n",
    "        di = get_results_from_pickle(file=file, publicPath=dirDataPrivate, privatePath=dirDataEpic)\n",
    "    logger.info('Loaded raw data.')\n",
    "    return di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_epic_data(di=None, file=None, convert=None):\n",
    "    '''Creates a dictionay of dataframes by vaccine and intervention timepoint\n",
    "\n",
    "        Args:\n",
    "            di (dict): Dictionary to clean.\n",
    "            file (str, Optional): Name of file for saving and retrieving\n",
    "                pickled data\n",
    "            convert (bool, Optional): Executes data cleaning and saves \n",
    "                results when True otherwise, data is read from pickle if \n",
    "                available. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            di (dict): Dictionary of dataframes\n",
    "\n",
    "    '''\n",
    "    if file == None:\n",
    "        file = retrieve_name(di) + '.p'\n",
    "    else:\n",
    "        file = file + '.p'\n",
    "    if (convert == True) | (os.path.isfile(os.path.join(dirDataEpic,\n",
    "                                                        file)) == False):\n",
    "        if di == None:\n",
    "            raise UnboundLocalError(\n",
    "                'Argument \"di\" must be passed when convert=True.')\n",
    "        logger.info('Processing data')\n",
    "        #     Merge in LOC_ID\n",
    "        di = fix_flu_files(di)\n",
    "        #   Add missing birthdays\n",
    "        di = concat_birthdays(di)\n",
    "        # Fix column labels\n",
    "        di = standardize_column_names(di)\n",
    "        # convert date columns to datetime\n",
    "        di = convert_dates(di)\n",
    "        # Convert variables to bools\n",
    "        di = convert_bools(di)\n",
    "        #  add additional data\n",
    "        di = append_additional_data(di)\n",
    "        #    Mark pre/post intervention records\n",
    "        di = add_timepoint_indicator(di)\n",
    "        # Strip department from practice name\n",
    "        di = split_practice_name(di)\n",
    "\n",
    "        save_results_to_pickle(di, file, privatePath=dirDataEpic, publicPath=dirDataPrivate)\n",
    "    else:\n",
    "        di = get_results_from_pickle(file=file, privatePath=dirDataEpic, \n",
    "                                     publicPath=dirDataPrivate)\n",
    "    return di\n",
    "\n",
    "\n",
    "def fix_flu_files(di):\n",
    "    '''Fills missing columns from values in available data.\n",
    "\n",
    "        Args:\n",
    "            di (dict): Dictionary of dataframes with missing columns\n",
    "\n",
    "        Returns:\n",
    "            di (dict): Dictionary of dataframes with missing columns appended and filled\n",
    "    '''\n",
    "    problemFiles = [\n",
    "        'dfFlu2014-15_AllLocs', 'dfFlu2017-18_AllLocs',\n",
    "        'dfFlu2014-15_AdditionalLocs', 'dfFlu2017-18_AdditionalLocs'\n",
    "    ]\n",
    "    dfPractices = pd.DataFrame()\n",
    "    # Collect LOC_IDs to fix flu files\n",
    "    for k, df in di.items():\n",
    "        if k not in problemFiles:\n",
    "            dfPractices = pd.concat([dfPractices, df[['LOC_ID', 'PRACTICE']]],\n",
    "                                    axis=0).drop_duplicates()\n",
    "    # Iterate through dict to fix the flu files\n",
    "    for k, df in di.items():\n",
    "        if k in problemFiles:\n",
    "            di[k] = di[k].merge(dfPractices)\n",
    "            logger.debug('Added LOC_ID to ' + k)\n",
    "    logger.info('Fixed missing data in flu files.')\n",
    "    return di\n",
    "\n",
    "\n",
    "def concat_birthdays(di):\n",
    "    '''Joins missing BIRTH_DATE column from available data.\n",
    "\n",
    "        Args: \n",
    "            di (dict): Dictionary of dataframes\n",
    "\n",
    "        Returns:\n",
    "            di (dict): Dictionary with BIRTH_DATE column joined to all dataframes\n",
    "    '''\n",
    "    problemFiles = ['dfPreIntervZoster_AllLOCs']\n",
    "    dfBirthdays = pd.DataFrame()\n",
    "    for k, df in di.items():\n",
    "        # Create list of birthdays\n",
    "        if k not in problemFiles:\n",
    "            dfBirthdays = pd.concat(\n",
    "                [dfBirthdays, df[['PAT_MRN_ID', 'BIRTH_DATE']]],\n",
    "                axis=0).drop_duplicates()\n",
    "    for k, df in di.items():\n",
    "        if k in problemFiles:\n",
    "            di[k] = di[k].merge(dfBirthdays)\n",
    "            logger.debug('Added birthdays to ' + k)\n",
    "    logger.info('Added missing birthdays.')\n",
    "    return di\n",
    "\n",
    "\n",
    "def standardize_column_names(di):\n",
    "    '''Applies map of old values: new values to make columns consistant.\n",
    "\n",
    "        Args:\n",
    "            di (dict): Dictionary of dataframes to be updated\n",
    "\n",
    "        Returns:\n",
    "            di (dict): Dict of dataframes with updated column labels\n",
    "    '''\n",
    "    mapper = {\n",
    "        'NUM_OF_ENCS_2014_15': 'NUM_OF_ENCS',\n",
    "        'NUM_OF_ENCS_2017_18': 'NUM_OF_ENCS',\n",
    "        'NUMBER_OF_ENCOUNTERS': 'NUM_OF_ENCS',\n",
    "        'LAST_FLU_DT_14_15': 'LAST_FLU_DT',\n",
    "        'LAST_FLU_DT_2017_18': 'LAST_FLU_DT',\n",
    "        \"FIRST_FLU_SEASON_VISIT\": \"FIRST_FLU_VISIT_DT\",\n",
    "        \"FIRST_BASELINE_VISIT\": \"FIRST_VISIT_DT\",\n",
    "        'FIRST_POSTINTERV_VISIT': \"FIRST_VISIT_DT\",\n",
    "        'ZOST_AT_A_VISIT': 'ZOST_AT_VISIT',\n",
    "        'FLUVAC_AT_A_VISIT': 'FLU_AT_VISIT',\n",
    "        'FLUVAC_AT_VISIT': 'FLU_AT_VISIT',\n",
    "        'FLUVAC_BEFORE_FIRST_VIS': 'FLU_BEFORE_FIRST_VIS',\n",
    "        'LAST_ZOSTER_DT': 'LAST_ZOST_DT'\n",
    "    }\n",
    "    for k, df in di.items():\n",
    "        di[k] = di[k].rename(mapper, axis='columns')\n",
    "        # Special treatment for flu encounters\n",
    "        if k in [\n",
    "                'dfFlu2014-15', 'dfFlu2017-18', 'dfFlu2014-15_AdditionalLocs',\n",
    "                'dfFlu2014-15_AllLocs', 'dfFlu2017-18_AdditionalLocs',\n",
    "                'dfFlu2017-18_AllLocs'\n",
    "        ]:\n",
    "            di[k] = di[k].rename({'NUM_OF_ENCS': 'NUM_FLU_ENCS'},\n",
    "                                 axis='columns')\n",
    "        di[k] = di[k].drop('LIVING_STATUS', axis=1)\n",
    "        logger.debug('Updated column labels for ' + k)\n",
    "    logger.info('Standardized column names.')\n",
    "    return di\n",
    "\n",
    "\n",
    "def convert_dates(di):\n",
    "    '''Converts text dates to datetime type.\n",
    "\n",
    "        Args:\n",
    "            di (dict): Dictionary of dataframes containing text dates\n",
    "\n",
    "        Returns:\n",
    "            di (dict): Dictionary of dataframes with converted datetimes\n",
    "    '''\n",
    "    # Variables to convert to dates\n",
    "    dateVars = [\n",
    "        'BIRTH_DATE', 'FIRST_VISIT_DT', 'FIRST_FLU_VISIT_DT', 'LAST_TDAP_DT',\n",
    "        'LAST_TD_DT', 'LAST_ZOST_DT', 'LAST_PNEUMO_DT', 'LAST_PCV_DT',\n",
    "        'LAST_FLU_DT'\n",
    "    ]\n",
    "    for k, df in di.items():\n",
    "        dateCols = [i for i in list(df) if i in dateVars]\n",
    "        df[dateCols] = df[dateCols].apply(pd.to_datetime)\n",
    "        logger.debug(str(dateCols) + ' converted to datetime.')\n",
    "    logger.info('Converted dates.')\n",
    "    return di\n",
    "\n",
    "\n",
    "def convert_bools(di):\n",
    "    '''Converts text values in dataframes to boolean True or False.\n",
    "\n",
    "        Args:\n",
    "            di (dict): Dictionary of dataframes with truth values stored as text\n",
    "\n",
    "        Returns:\n",
    "            di (dict): Dictionary of dataframes with boolean truth values\n",
    "    '''\n",
    "    # Variables to convert to booleans\n",
    "    boolVars = [\n",
    "        'AGE_60_64', 'AGE_65PLUS', 'ZOST_BEFORE_FIRST_VIS', 'ZOST_AT_VISIT',\n",
    "        'FLU_AT_VISIT', 'FLU_BEFORE_FIRST_VIS', 'PNEUMO_AT_VISIT',\n",
    "        'PNEUMO_BEFORE_FIRST_VIS', 'PCV_BEFORE_FIRST_VIS', 'PCV_AT_VISIT',\n",
    "        'TDAP_AT_VISIT', 'TDAP_BEFORE_FIRST_VIS', 'TD_BEFORE_FIRST_VIS',\n",
    "        'TD_AT_VISIT'\n",
    "    ]\n",
    "    # Build dictionaries of True / False strings\n",
    "    dictTF = {\n",
    "        'YES': True,\n",
    "        'NO': False,\n",
    "        'Y': True,\n",
    "        'N': False,\n",
    "        'True': True,\n",
    "        'False': False,\n",
    "        'NaN': False\n",
    "    }\n",
    "\n",
    "    # Build lists of T/F values\n",
    "    trues = [k for k, v in dictTF.items() if v == True]\n",
    "    falses = [k for k, v in dictTF.items() if v == False]\n",
    "    for k, df in di.items():\n",
    "        # perform conversion with error handling\n",
    "        for i in boolVars:\n",
    "            if i in df.columns:\n",
    "                countTrue = len(df[df[i] == True]) + len(df[df[i].isin(trues)])\n",
    "                countFalse = len(df[df[i] == False]) + len(\n",
    "                    df[df[i].isin(falses)]) + len(df[df[i].isnull()])\n",
    "                df[i] = df[i].fillna(False).astype(str).replace(dictTF).astype(\n",
    "                    bool)\n",
    "                df[i] = df[i].astype(bool)\n",
    "\n",
    "                # check results\n",
    "                countTrueAfter = len(df[df[i] == True])\n",
    "                countFalseAfter = len(df[df[i] == False])\n",
    "\n",
    "                assert(countTrue == countTrueAfter) & (countFalse == countFalseAfter) & \\\n",
    "                    (len(df[i]) == countTrueAfter + countFalseAfter), \\\n",
    "                    'T/F substitution error on ' + i\n",
    "                logger.debug('T/F substitution complete for ' + i)\n",
    "    logger.info('Converted boolean variables')\n",
    "    return di\n",
    "\n",
    "\n",
    "def append_additional_data(di):\n",
    "    '''Combines multiple data pulls into files by vaccine & intervention period.\n",
    "\n",
    "        Args:\n",
    "            di (dict): Dictionary of dataframes to be combined\n",
    "\n",
    "        Returns:\n",
    "            diNew (dict): New dictionary of dataframes combined per specification\n",
    "\n",
    "    '''\n",
    "    # These are the data files\n",
    "    matches = [('Flu2014-15_AdditionalLocs.xlsx', 'Flu2014-15_AllLocs.xlsx'),\n",
    "               ('Flu2017-18_AdditionalLocs.xlsx', 'Flu2017-18_AllLocs.xlsx'),\n",
    "               ('PostIntervPneumo_AdditionalLOCs.xlsx',\n",
    "                'PostIntervPneumo_AllLOCs.xlsx'),\n",
    "               ('PostIntervTDAP_TD_AdditionalLOCs.xlsx',\n",
    "                'PostIntervTDAP_TD_AllLOCs.xlsx'),\n",
    "               ('PostIntervZoster_AdditionalLOCs.xlsx',\n",
    "                'PostIntervZoster_AllLOCs.xlsx'),\n",
    "               ('PreIntervPneumo_AdditionalLOCs.xlsx',\n",
    "                'PreIntervPneumo_AllLOCs.xlsx'),\n",
    "               ('PreIntervTDAP_TD_AdditionalLOCs.xlsx',\n",
    "                'PreIntervTDAP_TD_AllLOCs.xlsx'),\n",
    "               ('PreIntervZoster_AdditionalLOCs.xlsx',\n",
    "                'PreIntervZoster_AllLOCs.xlsx')]\n",
    "    # rename pairs of data files\n",
    "    diNew = {}\n",
    "    for (additional, allLocs) in matches:\n",
    "        # rename keys for convenience\n",
    "        additional = 'df' + additional.replace('.xlsx', '')\n",
    "        allLocs = 'df' + allLocs.replace('.xlsx', '')\n",
    "        k = allLocs.replace('_AllLOCs', '').replace('_AllLocs', '')\n",
    "\n",
    "        # Record dtypes\n",
    "        diDtypes = {}\n",
    "        diDtypes.update(di[allLocs].dtypes.to_dict())\n",
    "        diDtypes.update(di[additional].dtypes.to_dict())\n",
    "        # Merge the two data files on common columns and add to a new dict\n",
    "        # Apply saved dtypes\n",
    "        diNew[k] = pd.concat([di[allLocs], di[additional]],\n",
    "                             ignore_index=True,\n",
    "                             sort=False,\n",
    "                             join='outer').drop_duplicates().apply(\n",
    "                                 lambda x: x.astype(diDtypes[x.name]))\n",
    "        logger.debug('Additional Data appended to ' + k)\n",
    "    logger.info('Combined additional data')\n",
    "    return diNew\n",
    "\n",
    "\n",
    "def add_timepoint_indicator(di):\n",
    "    '''Adds indicator for pre/post intervention identification.\n",
    "\n",
    "        Args:\n",
    "            di (dict): Dictionary of dataframes\n",
    "\n",
    "        Returns:\n",
    "            di (dict): Dictionary of dataframes with BASELINE column set\n",
    "                to True for pre-intervention and False for post-intervention\n",
    "    '''\n",
    "    for k, df in di.items():\n",
    "        #     assign baseline indicator\n",
    "        if k in [\n",
    "                'dfFlu2014-15', 'dfPreIntervPneumo', 'dfPreIntervTDAP_TD',\n",
    "                'dfPreIntervZoster'\n",
    "        ]:\n",
    "            df['BASELINE'] = True\n",
    "            logger.info(\n",
    "                'Added BASELINE=True intervention timepoint indicator to ' + k)\n",
    "        elif k in [\n",
    "                'dfFlu2017-18', 'dfPostIntervPneumo', 'dfPostIntervTDAP_TD',\n",
    "                'dfPostIntervZoster'\n",
    "        ]:\n",
    "            df['BASELINE'] = False\n",
    "            logger.info(\n",
    "                'Added BASELINE=False intervention timepoint indicator to ' +\n",
    "                k)\n",
    "    logger.info('Added timepoint indicator')\n",
    "    return di\n",
    "\n",
    "\n",
    "def split_practice_name(di):\n",
    "    '''Splits department id from practice name\n",
    "\n",
    "        Args:\n",
    "            di (dict): Dictionary of dataframes\n",
    "\n",
    "        Returns:\n",
    "            di (dict): Dictionary of dataframe with split practice names\n",
    "    '''\n",
    "    for k, df in di.items():\n",
    "        # Split pratice label to department id and name\n",
    "        di[k][['DEPT_ID', 'PRACTICE']] = df['PRACTICE'].str.split(\n",
    "            '-', expand=True)\n",
    "        di[k]['DEPT_ID'] = df['DEPT_ID'].astype('int')\n",
    "        logger.debug('Split practice names in ' + str(k))\n",
    "    logger.info('Split practice names')\n",
    "    return di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup_records(di, file=None, convert=None):\n",
    "    '''De-duplicates a dictionay of dataframes of vaccine and intervention\n",
    "        timepoints. Saves to .p and .csv\n",
    "\n",
    "        Args:\n",
    "            di (dict): Dictionary of processed Epic data.\n",
    "            file (str, Optional): File to save pickled data.\n",
    "            convert (bool, Optional): Executes data cleaning and saves results\n",
    "                when True. Otherwise, data is read from pickle if available.\n",
    "\n",
    "        Returns:\n",
    "            df (dataframe): Dataframe of combined, de-duplicated dataframes\n",
    "\n",
    "    '''\n",
    "    if file == None:\n",
    "        file = 'dfCombined.p'\n",
    "    else:\n",
    "        file = file + '.p'\n",
    "    if (convert == True) | (os.path.isfile(os.path.join(dirDataEpic,\n",
    "                                                        file)) == False):\n",
    "        # Combine to files by vaccine\n",
    "        di = combine_vaccine_data(di)\n",
    "        # de-duplicate data\n",
    "        df = process_columns(di)\n",
    "        logger.info('Deduplicated data.')\n",
    "        #  Save results\n",
    "        save_results_to_pickle(\n",
    "            obj=df,\n",
    "            file=file,\n",
    "            privatePath=dirDataEpic,\n",
    "            publicPath=dirDataPrivate)\n",
    "    else:\n",
    "        df = get_results_from_pickle(\n",
    "            file=file, privatePath=dirDataEpic, publicPath=dirDataPublic)\n",
    "    return df\n",
    "\n",
    "\n",
    "def combine_vaccine_data(di):\n",
    "    '''Combine files by vaccine and intervention timepoint into a dataframe by vaccine.\n",
    "\n",
    "        Args:\n",
    "            di (dict): Dictionary of dataframes to be combined\n",
    "\n",
    "        Returns:\n",
    "            df (df): Dataframes combined into one\n",
    "    '''\n",
    "    dfs = [('dfFlu2014-15', 'dfFlu2017-18', 'dfFlu'),\n",
    "           ('dfPreIntervPneumo', 'dfPostIntervPneumo', 'dfPneumo'),\n",
    "           ('dfPreIntervTDAP_TD', 'dfPostIntervTDAP_TD', 'dfTDAP'),\n",
    "           ('dfPreIntervZoster', 'dfPostIntervZoster', 'dfZoster')]\n",
    "    news = []\n",
    "    for pre_k, post_k, new_k in dfs:\n",
    "        # combine\n",
    "        di[new_k] = di[pre_k].append(di[post_k], sort=False).drop_duplicates()\n",
    "        # customize for flu\n",
    "        if 'Flu' in pre_k:\n",
    "            di[new_k] = di[new_k].rename(\n",
    "                columns={\n",
    "                    'NUM_OF_ENCS': 'NUM_FLU_ENCS',\n",
    "                    'FIRST_VISIT_DT': 'FIRST_FLU_VISIT_DT'\n",
    "                })\n",
    "        news.append(di[new_k])\n",
    "        logger.info('Combined and updated ' + new_k)\n",
    "    diNew = {}\n",
    "    df = pd.concat(news, sort=False).drop_duplicates()\n",
    "    logger.info('Combined all ')\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_columns(df):\n",
    "    '''Apply deduplication logic to specified dataframe by column.\n",
    "    \n",
    "        Args:\n",
    "            df (DataFrame): DataFrame to process by column.\n",
    "            \n",
    "        Returns:\n",
    "            df (DataFrame): Original DataFrame with per-column logic applied.\n",
    "    '''\n",
    "\n",
    "    # Iterate through the columns to identify and deduplicate by column\n",
    "    for col in df.columns.values:\n",
    "        # These are the permissible duplicates or handled elsewhere\n",
    "        if col in [\n",
    "                'PAT_MRN_ID', 'PRACTICE', 'AGE_60_65', 'LOC_ID', 'DEPT_ID',\n",
    "                'AGE_60_64', 'BASELINE'\n",
    "        ]:\n",
    "            logger.debug('Skipping ' + col)\n",
    "            pass\n",
    "        else:\n",
    "            logger.info('Processing ' + col)\n",
    "            # Fix patients who had a birthday\n",
    "            if col == 'AGE_65PLUS':\n",
    "                # Limit transformations to patients at timepoints\n",
    "                idx = ['BASELINE', 'PAT_MRN_ID']\n",
    "\n",
    "                # Assume all patients who turned 65 during the period,\n",
    "                # started the period at 65.\n",
    "                import operator\n",
    "                df.loc[:, col] = df.groupby(\n",
    "                    idx)[col].transform(lambda x: bool(x.max()))\n",
    "                df['AGE_60_64'] = df.groupby(\n",
    "                    idx)[col].transform(lambda x: bool(operator.not_(x.max())))\n",
    "\n",
    "            elif col in ['FIRST_VISIT_DT', 'FIRST_FLU_VISIT_DT']:\n",
    "                # Limit transformations to patients at timepoints at locations\n",
    "                idx = ['BASELINE', 'PAT_MRN_ID', 'LOC_ID']\n",
    "                # Set these columns to oldest available value\n",
    "                df[col] = df.groupby(idx)[col].transform(lambda x: x.min())\n",
    "\n",
    "            elif col in [\n",
    "                    'LAST_FLU_DT', 'NUM_OF_ENCS', 'NUM_FLU_ENCS',\n",
    "                    'LAST_PNEUMO_DT', 'LAST_PCV_DT', 'LAST_TDAP_DT',\n",
    "                    'LAST_TD_DT', 'LAST_ZOST_DT'\n",
    "            ]:\n",
    "                # Limit transformations to patients at timepoints at locations\n",
    "                idx = ['BASELINE', 'PAT_MRN_ID', 'LOC_ID']\n",
    "                # Set values to max available value for these columns\n",
    "                df[col] = df.groupby(idx)[col].transform(lambda x: x.max())\n",
    "\n",
    "            elif col in [\n",
    "                    'FLU_AT_VISIT', 'FLU_BEFORE_FIRST_VIS', 'PNEUMO_AT_VISIT',\n",
    "                    'PCV_AT_VISIT', 'PNEUMO_BEFORE_FIRST_VIS',\n",
    "                    'PCV_BEFORE_FIRST_VIS', 'TDAP_AT_VISIT', 'TD_AT_VISIT',\n",
    "                    'TDAP_BEFORE_FIRST_VIS', 'TD_BEFORE_FIRST_VIS',\n",
    "                    'ZOST_AT_VISIT', 'ZOST_BEFORE_FIRST_VIS'\n",
    "            ]:\n",
    "                # Limit transformations to patients at timepoints at locations\n",
    "                idx = ['BASELINE', 'PAT_MRN_ID', 'LOC_ID']\n",
    "                # Set the column to true if any entry is true for the group\n",
    "                df[col] = df.groupby(\n",
    "                    idx)[col].transform(lambda x: bool(x.max()))\n",
    "\n",
    "            elif col == 'BIRTH_DATE':\n",
    "\n",
    "                def fn(group):\n",
    "                    # check values\n",
    "                    assert group[col].max() == group[col].min(\n",
    "                    ), 'errors in birthday'\n",
    "\n",
    "                df.groupby(['PAT_MRN_ID']).apply(fn)\n",
    "            else:\n",
    "                logger.debug(\"No logic for \" + col)\n",
    "\n",
    "    return df.drop_duplicates().sort_values(\n",
    "        ['BASELINE', 'LOC_ID', 'DEPT_ID', 'PAT_MRN_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Site data cleaning helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_intervention_list(di, convert=None):\n",
    "    '''Creates a datafame with site information.\n",
    "\n",
    "        Args:\n",
    "            di (dict): Dictionary of Epic data.\n",
    "            convert (bool, Optional): Executes data cleaning and saves \n",
    "            results when True otherwise, data is read from pickle if available.\n",
    "            Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            dfSites (dataframe): Merge of site information table with data\n",
    "            from Epic records        \n",
    "    '''\n",
    "    # Fetch Site information\n",
    "    file = 'dfSites.p'\n",
    "    if (convert == True) | (os.path.isfile(os.path.join(dirDataPublic,\n",
    "                                                        file)) == False):\n",
    "        dfSites = clean_intervention_dates_sheet()\n",
    "        dfSites = create_practice_list(di, dfSites)\n",
    "        #  Save results\n",
    "        save_results_to_pickle(\n",
    "            obj=dfSites,\n",
    "            file=file,\n",
    "            privatePath=dirDataEpic,\n",
    "            publicPath=dirDataPrivate)\n",
    "    else:\n",
    "        dfSites = get_results_from_pickle(\n",
    "            file=file, privatePath=dirDataEpic, publicPath=dirDataPublic)\n",
    "    return dfSites\n",
    "\n",
    "\n",
    "def clean_intervention_dates_sheet():\n",
    "    '''Imports and cleans intervention dates and Epic location worksheet\n",
    "    \n",
    "        Returns:\n",
    "            dfSites (dataframe): Dataframe from Excel worksheet\n",
    "    '''\n",
    "    cols = [\n",
    "        'LOC_ID', 'LOC', 'Clinic Location Name', 'Street Address', 'PHASE',\n",
    "        'Date of intervention', 'Notes'\n",
    "    ]\n",
    "    dfSites = pd.read_excel(\n",
    "        os.path.join(dirDataPrivate,\n",
    "                     'Epic ID and Intervention Dates 4 Pillars.xlsx'))\n",
    "\n",
    "    dfSites.columns = dfSites.columns.str.strip()\n",
    "    dfSites = dfSites[cols].rename(columns={\n",
    "            \"LOC\": \"DEPT_ID\"\n",
    "        }).reset_index(drop=True)\n",
    "    dfSites = dfSites.dropna(subset=['DEPT_ID'])\n",
    "    dfSites['DEPT_ID'] = dfSites['DEPT_ID'].astype('int')\n",
    "    return dfSites\n",
    "\n",
    "\n",
    "def create_practice_list(di, dfSites):\n",
    "    '''Joins list of unique departments and practices found in Epic data with\n",
    "    intervention dates and locations.\n",
    "    \n",
    "        Args:\n",
    "            di (dict): Dictionary of dataframes from Epic export\n",
    "            dfSites (dataframe): Dataframe with department, locations and practice names\n",
    "            \n",
    "        Returns:\n",
    "            dfPractices (dataframe): Dataframe of unique departments and practices joind\n",
    "            with intervention dates and locations\n",
    "    '''\n",
    "    # Create list of unique departments, locations & practices found in the data\n",
    "    dfPractices = pd.DataFrame()\n",
    "    for k, df in di.items():\n",
    "        df2 = df.groupby(['DEPT_ID', 'LOC_ID',\n",
    "                          'PRACTICE']).size().reset_index(name='Freq').drop(\n",
    "                              'Freq', axis=1)\n",
    "        dfPractices = dfPractices.append(df2, ignore_index=True)\n",
    "    # Merge Pracitces from Epic with Intervention list\n",
    "    dfPractices = dfPractices.merge(\n",
    "        dfSites, on=['LOC_ID', 'DEPT_ID'], how='outer').sort_values(\n",
    "            by=['LOC_ID', 'DEPT_ID']).drop_duplicates().reset_index(drop=True)\n",
    "    dfPractices = dfPractices.dropna(subset=['PRACTICE'])\n",
    "    logger.info('Practice info processed')\n",
    "    return dfPractices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_calculate_site_data(df, dfSites, file=None, convert=None):\n",
    "    '''Creates a dictionay of dataframes with cleaned sites and reduced to location.\n",
    "\n",
    "        Args:\n",
    "            df (dataframe): Dataframe of processed Epic data.\n",
    "            file (str, Optional): File to save pickled data.\n",
    "            convert (bool, Optional): Executes data cleaning and saves \n",
    "                results when True otherwise, data is read from pickle if available.\n",
    "\n",
    "        Returns:\n",
    "            di (dict): Dictionary of dataframes aggregated to location \n",
    "\n",
    "    '''\n",
    "    if file == None:\n",
    "        file = retrieve_name(df) + '.p'\n",
    "    else:\n",
    "        file = file + '.p'\n",
    "    if (convert == True) | (os.path.isfile(os.path.join(dirDataEpic,\n",
    "                                                        file)) == False):\n",
    "        logger.info('Processing data')\n",
    "\n",
    "        dfProbs = dfSites[\n",
    "            dfSites['Date of intervention'].isnull()].reset_index()[[\n",
    "                'LOC_ID', 'DEPT_ID', 'PRACTICE'\n",
    "            ]]\n",
    "        startingProbs = questionable_departments(df, dfProbs)\n",
    "        # Update site records\n",
    "        df = update_site_names(df, diUpdates=dicts.diUpdates)\n",
    "        # Drop records from excluded sites\n",
    "        df = drop_site_records(df, diDrops=dicts.diDrops)\n",
    "\n",
    "        endingProbs = questionable_departments(df, dfProbs)\n",
    "        assert len(startingProbs) > len(endingProbs), 'Problem with cleaning'\n",
    "        df = compute_vaccine_logic(df)\n",
    "        di = reduce_to_locations(df, diOmit=dicts.diOmit)\n",
    "        # include sites in dict\n",
    "        di['dfSites'] = dfSites\n",
    "        #  Save results\n",
    "        save_results_to_pickle(obj=di, file=file, publicPath=dirDataPrivate, privatePath=dirDataEpic)\n",
    "    else:\n",
    "        di = get_results_from_pickle(file=file, publicPath=dirDataPrivate, privatePath=dirDataEpic)\n",
    "    return di\n",
    "\n",
    "\n",
    "def questionable_departments(df, dfProbs):\n",
    "    '''Shows records aggregated by count for specified departments\n",
    "\n",
    "        Args:\n",
    "            df (dataframe): Dataframe with questionable records by department.\n",
    "            dfProbs (dataframe): Dataframe of problem departments. Must \n",
    "                include 'DEPT_ID' column\n",
    "\n",
    "        Returns:\n",
    "            dfGrouped (dataframe): Dataframe of records indexed and grouped\n",
    "                for inspection. Displays when logger is set to DEBUG.\n",
    "\n",
    "    '''\n",
    "    # All records with a Department problem\n",
    "    idx = ['LOC_ID', 'BASELINE', 'DEPT_ID', 'PRACTICE']\n",
    "    dfProbView = df[df.DEPT_ID.isin(\n",
    "        dfProbs['DEPT_ID'])].drop_duplicates().reset_index().set_index(\n",
    "            idx).sort_index()\n",
    "    # Groupby location/Timepoint\n",
    "    dfGrouped = dfProbView.reset_index().groupby(idx)['PAT_MRN_ID'].agg(\n",
    "        [\"count\"])\n",
    "    return dfGrouped\n",
    "\n",
    "\n",
    "def update_site_names(df, diUpdates):\n",
    "    '''Replaces site names and ids.\n",
    "\n",
    "        Args:\n",
    "            df (dataframe): Dataframe with records to be updated\n",
    "            diUpdates (dictionary): Dictonary of values in the format \n",
    "                <old>:<new> to be replaced\n",
    "        Returns:\n",
    "            df (dataframe): Dataframe with updated names and ids\n",
    "    '''\n",
    "    # Update site records\n",
    "    df = df.replace(to_replace=diUpdates)\n",
    "    # Check that all sites were removed\n",
    "    assert len(df.query(\n",
    "        'DEPT_ID.isin(@diUpdates.keys())')) == 0, 'Failed to update all sites.'\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_site_records(df, diDrops):\n",
    "    '''Drops records indexed to specified sites.\n",
    "\n",
    "        Args:\n",
    "            df (dataframe): Dataframe with records to be updated\n",
    "            diDrops (dictionary): Dictonary of sites with data to drop in the format \n",
    "                'DEPT_ID': 'PRACTICE'\n",
    "        Returns:\n",
    "            df (dataframe): Dataframe with records dropped by site\n",
    "    '''\n",
    "    # Update site records\n",
    "    df = df.query('~DEPT_ID.isin(@diDrops.keys())')\n",
    "    # Check that all sites were removed\n",
    "    assert len(df.query(\n",
    "        'DEPT_ID.isin(@diDrops.keys())')) == 0, 'Problem dropping records.'\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_vaccine_logic(df):\n",
    "    '''Computes vaccination variables for patients.\n",
    "    \n",
    "        Args:\n",
    "            df (dataframe): Dataframe of processesd data\n",
    "                \n",
    "        Returns:\n",
    "            df (dataframe): Dataframe of processed data with vaccine logic applied\n",
    "    '''\n",
    "    vaxVars = ['FLU', 'PNEUMO', 'PCV', 'TDAP', 'TD', 'ZOST']\n",
    "    for vax in vaxVars:\n",
    "        firstVisit = df['FIRST_VISIT_DT']\n",
    "        numEncs = df['NUM_OF_ENCS']\n",
    "        age65 = df['AGE_65PLUS']\n",
    "        vaxBefore = df[vax + '_BEFORE_FIRST_VIS']\n",
    "        lastVaxDate = df['LAST_' + vax + '_DT']\n",
    "        vaxHere = df[vax + '_AT_VISIT']\n",
    "        if vax in ['FLU']:\n",
    "            # Use flu variables\n",
    "            firstVisit = df['FIRST_FLU_VISIT_DT']\n",
    "            numEncs = df['NUM_FLU_ENCS']\n",
    "            # Assign vaccine eligibility to all unvaccinated\n",
    "            df[vax + '_ELIGIBLE'] = np.where(vaxBefore == False, True, False)\n",
    "        elif vax in ['PNEUMO', 'PCV']:\n",
    "            # Assign vaccine eligibility to all unvaccinated over 65\n",
    "            df[vax + '_ELIGIBLE'] = np.where(\n",
    "                ((vaxBefore == False) & (age65 == True)), True, False)\n",
    "        elif vax in ['TDAP']:\n",
    "            # Assign vaccine eligibility to all unvaccinated\n",
    "            df[vax + '_ELIGIBLE'] = np.where((vaxBefore == False), True, False)\n",
    "        elif vax in ['TD']:\n",
    "            # Assign vaccine eligibility to all unvaccinated, with a Tdap >10 years ago\n",
    "            df[vax + '_ELIGIBLE'] = np.where(\n",
    "                ((vaxBefore == False) &\n",
    "                 ((df['LAST_TDAP_DT'] <\n",
    "                   firstVisit - datetime.timedelta(days=3650)) == True)) |\n",
    "                (((lastVaxDate <\n",
    "                   firstVisit - datetime.timedelta(days=3650)) == True) &\n",
    "                 ((df['LAST_TDAP_DT'] <\n",
    "                   firstVisit - datetime.timedelta(days=3650)) == True)), True,\n",
    "                False)\n",
    "        elif vax in ['ZOST']:\n",
    "            # Assign vaccine eligibility to all unvaccinated\n",
    "            df[vax + '_ELIGIBLE'] = np.where(vaxBefore == False, True, False)\n",
    "            # Assign vaccine eligibility to all over 60\n",
    "        else:\n",
    "            print('Missing logic for ' + vax)\n",
    "            break\n",
    "        # All vaccine logic\n",
    "        eligible = df[vax + '_ELIGIBLE']\n",
    "        # Find vaccines administered elsewhere\n",
    "        df[vax + '_VACC_ELSEWHERE'] = np.where(\n",
    "            (lastVaxDate > firstVisit) & (vaxHere == False), True, False)\n",
    "        vaxElsewhere = df[vax + '_VACC_ELSEWHERE']\n",
    "        # Assign vaccinated status for vaccine administrations anywhere\n",
    "        df[vax + '_VACCINATED'] = np.where(\n",
    "            (vaxElsewhere == True) | (vaxHere == True), True, False)\n",
    "        vaccinated = df[vax + '_VACCINATED']\n",
    "        # Assign missed opportunities for eligible unvaccinated\n",
    "        df[vax + '_MISSED_OPS'] = np.where(\n",
    "            (eligible == True) & (vaccinated == False), numEncs, np.nan)\n",
    "    return df\n",
    "\n",
    "\n",
    "def reduce_to_locations(df, diOmit):\n",
    "    '''Excludes non-primary care visits using two different strategies.\n",
    "        \n",
    "        Description:\n",
    "            Strategy 1 returns dfWithWic which includes patients seen in \n",
    "            walk-in-clinics in location visit counts. It assumes that vaccine\n",
    "            status is prioritized for these patients and penalizes sites for \n",
    "            missing opportunities at these visits.\n",
    "            \n",
    "            Strategy 2 returns dfNoWic which excludes patients seen in \n",
    "            walk-in-clinics from location visit counts. It assumes that vaccine\n",
    "            status is not prioritized for these patients and does not penalize \n",
    "            sites for missing opportunities at these visits.\n",
    "\n",
    "        Args:\n",
    "            df (dataframe): Dataframe with records to be updated\n",
    "            diOmit (dict): Dictonary of sites to omit in the format \n",
    "                'DEPT_ID': 'PRACTICE' \n",
    "\n",
    "        Returns:\n",
    "            di (dict): Dictionary with two dataframes produced by strategies\n",
    "    '''\n",
    "    di = {}\n",
    "    di['dfCombined'] = df\n",
    "\n",
    "    # Strategy 1\n",
    "    # Drop and deduplicate\n",
    "    dfWithWic = df.drop(['DEPT_ID', 'PRACTICE'], axis=1).drop_duplicates()\n",
    "    di['dfWithWic'] = dfWithWic\n",
    "\n",
    "    # Strategy 2\n",
    "    dfNoWic = df.query('~DEPT_ID.isin(@diOmit.keys())')\n",
    "    # Check that all sites were removed\n",
    "    assert len(dfNoWic.query(\n",
    "        'DEPT_ID.isin(@diOmit.keys())')) == 0, 'Incorrect sites included.'\n",
    "    # Now drop to LOC_ID\n",
    "    dfNoWic = dfNoWic.drop(['DEPT_ID', 'PRACTICE'], axis=1).drop_duplicates()\n",
    "    di['dfNoWic'] = dfNoWic\n",
    "    \n",
    "    # Strategy 3\n",
    "    lsOmit = [11207, 1145, 11156, 11131]\n",
    "    dfDropWic = df.query('~LOC_ID.isin(@lsOmit)')\n",
    "    assert len(dfDropWic.query('LOC_ID.isin(@lsOmit)')) == 0, 'Incorrect sites included'\n",
    "    # Now drop to LOC_ID\n",
    "    dfDropWic = dfDropWic.drop(['DEPT_ID', 'PRACTICE'], axis=1).drop_duplicates()\n",
    "    di['dfDropWic'] = dfDropWic\n",
    "    return di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_and_calculate(di, file=None, convert=None):\n",
    "    '''Creates a dictionay of dataframes aggregated to levels with outcomes.\n",
    "\n",
    "        Args:\n",
    "            di (dict): Dictionary of processed Epic data.\n",
    "            file (str, Optional): File to save pickled data.\n",
    "            convert (bool, Optional): Executes data cleaning and saves \n",
    "                results when True otherwise, data is read from pickle if available.\n",
    "\n",
    "        Returns:\n",
    "            di (dict): Dictionary of dataframes aggregated with outcomes\n",
    "\n",
    "    '''\n",
    "    if file == None:\n",
    "        file = retrieve_name(di) + '.p'\n",
    "    else:\n",
    "        file = file + '.p'\n",
    "    if (convert == True) | (os.path.isfile(os.path.join(dirDataEpic,\n",
    "                                                        file)) == False):\n",
    "        # Execute logic\n",
    "        di = aggregate_to_locations(di)\n",
    "        di = compute_outcomes(di)\n",
    "\n",
    "        logger.info('Aggregated data and calculated outcomes.')\n",
    "        #  Save results\n",
    "        save_results_to_pickle(\n",
    "            obj=di,\n",
    "            file=file,\n",
    "            privatePath=dirDataEpic,\n",
    "            publicPath=dirDataPrivate)\n",
    "    else:\n",
    "        di = get_results_from_pickle(\n",
    "            file=file, privatePath=dirDataEpic, publicPath=dirDataPublic)\n",
    "    return di\n",
    "\n",
    "\n",
    "def aggregate_to_locations(di):\n",
    "    '''Aggregates from patient level to location level.\n",
    "    \n",
    "        Args:\n",
    "            di (dict): Dictionary of processed data\n",
    "            \n",
    "        Returns:\n",
    "            di (dict): Dictionary of processed data with aggregated dataframes\n",
    "    '''\n",
    "    for k, df in copy.copy(di).items():\n",
    "        \n",
    "        if 'Wic' in k:\n",
    "            aggFxns = {\n",
    "                'PAT_MRN_ID': 'count',\n",
    "                'AGE_60_64': 'sum',\n",
    "                'AGE_65PLUS': 'sum',\n",
    "                'FIRST_VISIT_DT': 'min',\n",
    "                'NUM_OF_ENCS': 'sum',\n",
    "                'FIRST_FLU_VISIT_DT': 'min',\n",
    "                'NUM_FLU_ENCS': 'sum',\n",
    "                'LAST_FLU_DT': 'max',\n",
    "                'FLU_AT_VISIT': 'sum',\n",
    "                'FLU_VACC_ELSEWHERE': 'sum',\n",
    "                'FLU_ELIGIBLE': 'sum',\n",
    "                'FLU_VACCINATED': 'sum',\n",
    "                'FLU_MISSED_OPS': 'sum',\n",
    "                'LAST_PNEUMO_DT': 'max',\n",
    "                'PNEUMO_AT_VISIT': 'sum',\n",
    "                'PNEUMO_VACC_ELSEWHERE': 'sum',\n",
    "                'PNEUMO_ELIGIBLE': 'sum',\n",
    "                'PNEUMO_VACCINATED': 'sum',\n",
    "                'PNEUMO_MISSED_OPS': 'sum',\n",
    "                'LAST_PCV_DT': 'max',\n",
    "                'PCV_AT_VISIT': 'sum',\n",
    "                'PCV_VACC_ELSEWHERE': 'sum',\n",
    "                'PCV_ELIGIBLE': 'sum',\n",
    "                'PCV_VACCINATED': 'sum',\n",
    "                'PCV_MISSED_OPS': 'sum',\n",
    "                'LAST_TDAP_DT': 'max',\n",
    "                'TDAP_AT_VISIT': 'sum',\n",
    "                'TDAP_VACC_ELSEWHERE': 'sum',\n",
    "                'TDAP_ELIGIBLE': 'sum',\n",
    "                'TDAP_VACCINATED': 'sum',\n",
    "                'TDAP_MISSED_OPS': 'sum',\n",
    "                'LAST_TD_DT': 'max',\n",
    "                'TD_AT_VISIT': 'sum',\n",
    "                'TD_VACC_ELSEWHERE': 'sum',\n",
    "                'TD_ELIGIBLE': 'sum',\n",
    "                'TD_VACCINATED': 'sum',\n",
    "                'TD_MISSED_OPS': 'sum',\n",
    "                'LAST_ZOST_DT': 'max',\n",
    "                'ZOST_AT_VISIT': 'sum',\n",
    "                'ZOST_VACC_ELSEWHERE': 'sum',\n",
    "                'ZOST_ELIGIBLE': 'sum',\n",
    "                'ZOST_VACCINATED': 'sum',\n",
    "                'ZOST_MISSED_OPS': 'sum'\n",
    "            }\n",
    "            # Aggregate outcomes by timepoint and location\n",
    "            dfOutcomesLocs = df.groupby(\n",
    "                ['BASELINE', 'LOC_ID'],\n",
    "                sort=True).agg(aggFxns).reset_index()\n",
    "            dfOutcomesLocs.rename(columns={\n",
    "                    'PAT_MRN_ID': 'N_PATIENT'\n",
    "                }, inplace=True)\n",
    "            # save to dict\n",
    "            di.update({k + '_Outcomes_Locs': dfOutcomesLocs})\n",
    "            # Aggregate outcomes by Timepoint\n",
    "            aggFxns.update({'LOC_ID': 'count'})\n",
    "            dfOutcomesBaseline = df.groupby('BASELINE').agg(\n",
    "                aggFxns).reset_index()\n",
    "            dfOutcomesBaseline.rename(columns={\n",
    "                    'PAT_MRN_ID': 'N_PATIENT',\n",
    "                    'LOC_ID': 'N_LOC'\n",
    "                }, inplace=True)\n",
    "            # save to dict\n",
    "            di.update({k + '_Outcomes_Baseline': dfOutcomesBaseline})\n",
    "        elif k == 'dfSites':\n",
    "            # Aggregate sites to locations\n",
    "            dfSites_Locs = df.groupby('LOC_ID').agg({\n",
    "                'PRACTICE': 'first',\n",
    "                'Date of intervention': 'min',\n",
    "                'PHASE': 'min'\n",
    "            }).reset_index()\n",
    "            # save to dict\n",
    "            di.update({'dfSites_Locs': dfSites_Locs})\n",
    "    return di\n",
    "\n",
    "\n",
    "def compute_outcomes(di):\n",
    "    '''Computes vaccination outcomes for aggregated groups.\n",
    "    \n",
    "        Args:\n",
    "            di (dict): Dictionary of processesd data\n",
    "            vaxVars (list): List of vaccine strings used to identify outcomes\n",
    "                and data columns\n",
    "                \n",
    "        Returns:\n",
    "            di (dict): Dictionary of processed data with vaccine outcomes\n",
    "    '''\n",
    "    vaxVars = ['FLU', 'PNEUMO', 'PCV', 'TDAP', 'TD', 'ZOST']\n",
    "    for k, df in di.items():\n",
    "        if '_Outcomes_Locs' in k:\n",
    "            for vax in vaxVars:\n",
    "                df[vax +\n",
    "                   '_VAX_RATE'] = df[vax + '_VACCINATED'] / df['N_PATIENT']\n",
    "                if vax == 'FLU':\n",
    "                    df['FLU_MISSED_OPS_RATE'] = df['FLU_MISSED_OPS'] / df[\n",
    "                        'NUM_FLU_ENCS']\n",
    "                else:\n",
    "                    df[vax + '_MISSED_OPS_RATE'] = df[\n",
    "                        vax + '_MISSED_OPS'] / df['NUM_OF_ENCS']\n",
    "                di.update({k: df})\n",
    "    return di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_presentable(di, file=None, convert=None):\n",
    "    '''Creates a dictionay of dataframes conformed to tidy data structure\n",
    "\n",
    "    Args:\n",
    "        di (dict): Dictionary of dataframes aggregated with outcomes\n",
    "        file (str, Optional): File to save pickled data.\n",
    "        convert (bool, Optional): Executes data cleaning and saves \n",
    "            results when True otherwise, data is read from pickle if available.\n",
    "\n",
    "    Returns:\n",
    "        di (dict): Dictionary of dataframes conformed to tidy data structure\n",
    "\n",
    "    '''\n",
    "    if file == None:\n",
    "        file = retrieve_name(di) + '.p'\n",
    "    else:\n",
    "        file = file + '.p'\n",
    "    if (convert == True) | (os.path.isfile(os.path.join(dirDataEpic,\n",
    "                                                        file)) == False):\n",
    "        # Execute logic\n",
    "        for k, df in di.items(): \n",
    "            if k not in ['dfSites', 'dfSites_Locs']:\n",
    "                df = tidy_data(df)\n",
    "                df = make_readable(df)\n",
    "                di.update({k:df})\n",
    "        logger.info('Made data tidy.')\n",
    "        #  Save results\n",
    "        save_results_to_pickle(\n",
    "            obj=di,\n",
    "            file=file,\n",
    "            privatePath=dirDataEpic,\n",
    "            publicPath=dirDataPrivate)\n",
    "    else:\n",
    "        di = get_results_from_pickle(\n",
    "            file=file, privatePath=dirDataEpic, publicPath=dirDataPublic)\n",
    "    return di\n",
    "\n",
    "def tidy_data(df):\n",
    "    df['AGE_60_64'] = df['AGE_60_64'].replace({True:'60-64', False:'65+'})\n",
    "    df=df.drop(['AGE_65PLUS'], axis=1).rename(columns={'AGE_60_64': 'Age'})\n",
    "    return df\n",
    "\n",
    "def make_readable(df):\n",
    "    df['BASELINE'] = df['BASELINE'].replace({True: 'Baseline', False: 'Follow-up'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(convert=None):\n",
    "    '''Completes data procesing and returns objects for analysis.\n",
    "    \n",
    "        Args:\n",
    "            convert (bool, Optional, Default: False): Specifies conversion from\n",
    "                raw data when true. Otherwise, data is loaded from storage.    \n",
    "    '''\n",
    "    if convert == None:\n",
    "        convert = False\n",
    "\n",
    "    di = ingest_raw_data(file='rawData', convert=convert)\n",
    "    di = clean_epic_data(di, file='di', convert=convert)\n",
    "    dfSites = clean_intervention_list(di, convert=convert)\n",
    "    dfCombined = dedup_records(di, file='dfCombined', convert=convert)\n",
    "    di2 = clean_and_calculate_site_data(\n",
    "        dfCombined, dfSites=dfSites, file='di2', convert=convert)\n",
    "    di2 = aggregate_and_calculate(di2, file='di3', convert=convert)\n",
    "    di2 = make_presentable(di2, file='di4', convert=convert)\n",
    "    logger.info('\\n DataFrames available:\\n' + str(list(di2)))\n",
    "    return di2\n",
    "\n",
    "\n",
    "def save_results_to_pickle(obj, file, privatePath, publicPath=None):\n",
    "    '''Saves function results to specified pickle file.\n",
    "    \n",
    "        Args:\n",
    "            obj (object): Object to pickle\n",
    "            file (str): Name to use for the saved file.\n",
    "            privatePath (str path): Path to local storage directory.\n",
    "            publicPath (str path, Optional): Internet-accessible storage\n",
    "                directory.\n",
    "    '''\n",
    "    pickle.dump(obj, open(os.path.join(privatePath, file), 'wb'))\n",
    "    logger.debug('Saved ' + os.path.join(privatePath, file))\n",
    "    if publicPath != None:\n",
    "        pickle.dump(obj, open(os.path.join(publicPath, file), 'wb'))\n",
    "        logger.debug('Saved ' + os.path.join(dirDataPublic, file))\n",
    "\n",
    "\n",
    "def get_results_from_pickle(file, privatePath, publicPath=None):\n",
    "    '''Retrieves pickle saved by sister function save_results_to_pickle\n",
    "    \n",
    "        Args:\n",
    "            file (str): Name the saved file.\n",
    "            privatePath (str path): Path to local storage directory.\n",
    "            publicPath (str path, Optional): Internet-accessible storage\n",
    "                directory.\n",
    "        Returns:\n",
    "            obj (object): Object loaded from pickle\n",
    "    '''\n",
    "    try:\n",
    "        obj = pickle.load(open(os.path.join(privatePath, file), 'rb'))\n",
    "        logger.info('Using stored data from ' +\n",
    "                    os.path.join(privatePath, file))\n",
    "    except error as e:\n",
    "        logger.debug(e)\n",
    "        try:\n",
    "            obj = pickle.load(open(os.path.join(publicPath, file), 'rb'))\n",
    "            logger.info('Using stored data from ' +\n",
    "                        os.path.join(publicPath, file))\n",
    "        except error as e:\n",
    "            logger.debug(e)\n",
    "            logger.info('No stored data available.')\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patient_counts(df):\n",
    "    idx = ['LOC_ID', 'BASELINE', 'DEPT_ID', 'PRACTICE']\n",
    "    # https://stackoverflow.com/questions/15570099/pandas-pivot-tables-row-subtotals\n",
    "    try:\n",
    "        # Groupby practice and aggregate with count\n",
    "        grouped1 = df.groupby(idx)['PAT_MRN_ID'].agg(['count']).reset_index()\n",
    "        # Groupby location and aggregate with count\n",
    "        grouped2 = df.groupby(\n",
    "            idx[:-3])['PAT_MRN_ID'].agg(['count']).reset_index()\n",
    "        # Add columns so we can append and index\n",
    "        grouped2['BASELINE'] = '**Total**'\n",
    "        grouped = grouped2.append(grouped1, sort=True).set_index(\n",
    "            idx).sort_index().rename(columns={'count': 'Patient count'})\n",
    "    except:\n",
    "        idx = idx[:-2]\n",
    "        # Groupby location and aggregate with count\n",
    "        grouped1 = df.groupby(idx)['PAT_MRN_ID'].agg(['count']).reset_index()\n",
    "        grouped2 = df.groupby(\n",
    "            idx[:-1])['PAT_MRN_ID'].agg(['count']).reset_index()\n",
    "        # Add columns so we can append and index\n",
    "        grouped2['BASELINE'] = '**Total**'\n",
    "        # Append higher grouping and use alpha sort to position total row\n",
    "        grouped = grouped2.append(grouped1, sort=True).set_index(\n",
    "            idx).sort_index().rename(columns={'count': 'Patient count'})\n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova_table(aov):\n",
    "    aov['mean_sq'] = aov[:]['sum_sq']/aov[:]['df']\n",
    "    aov['eta_sq'] = aov[:-1]['sum_sq']/sum(aov['sum_sq'])\n",
    "    aov['omega_sq'] = (aov[:-1]['sum_sq']-(aov[:-1]['df'] *\n",
    "                                           aov['mean_sq'][-1]))/(sum(aov['sum_sq'])+aov['mean_sq'][-1])\n",
    "    cols = ['sum_sq', 'df', 'mean_sq', 'F', 'PR(>F)', 'eta_sq', 'omega_sq']\n",
    "    aov = aov[cols]\n",
    "    return aov"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
